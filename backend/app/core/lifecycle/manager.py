"""
Token lifecycle manager
–ú–µ–Ω–µ–¥–∂–µ—Ä –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –§–ó
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from decimal import Decimal

from app.models.token import TokenStatus, StatusChangeReason
from app.core.data_sources.birdeye_client import birdeye_manager

logger = logging.getLogger(__name__)


class TokenLifecycleManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Ç–æ–∫–µ–Ω–æ–≤
    
    –û—Ç–≤–µ—á–∞–µ—Ç –∑–∞:
    - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Initial —Ç–æ–∫–µ–Ω–æ–≤
    - –ê–∫—Ç–∏–≤–∞—Ü–∏—é –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ —É—Å–ª–æ–≤–∏–π
    - –ê—Ä—Ö–∏–≤–∞—Ü–∏—é –ø–æ —Ç–∞–π–º–∞—É—Ç—É
    - –î–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—é –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    """
    
    def __init__(self):
        self.stats = {
            "initial_tokens_checked": 0,
            "active_tokens_checked": 0,
            "tokens_activated": 0,
            "tokens_archived": 0,
            "tokens_deactivated": 0,
            "last_check": None,
            "errors": 0
        }

    async def monitor_initial_tokens(self, db_session) -> Dict[str, Any]:
        """
        –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å—Ç–∞—Ç—É—Å–µ Initial
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
        1. –ü–æ—è–≤–ª–µ–Ω–∏–µ –ø—É–ª–æ–≤ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å—é
        2. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
        3. –¢–∞–π–º–∞—É—Ç –¥–ª—è –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏
        """
        try:
            from app.crud import token_crud, system_crud
            
            logger.info("Monitoring Initial tokens...")
            
            # –ü–æ–ª—É—á–∞–µ–º Initial —Ç–æ–∫–µ–Ω—ã
            initial_tokens = token_crud.get_by_status(
                db_session,
                status=TokenStatus.INITIAL,
                limit=1000  # –ë–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –¥–ª—è Initial
            )
            
            self.stats["initial_tokens_checked"] = len(initial_tokens)
            
            if not initial_tokens:
                logger.info("No Initial tokens found")
                return {
                    "status": "success",
                    "tokens_processed": 0,
                    "message": "No Initial tokens found"
                }
            
            logger.info(f"Found {len(initial_tokens)} Initial tokens to check")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            min_liquidity = float(system_crud.get_value(
                db_session,
                key="MIN_LIQUIDITY_USD",
                default=500
            ))
            
            min_tx_count = int(system_crud.get_value(
                db_session,
                key="MIN_TX_COUNT", 
                default=300
            ))
            
            archival_timeout_hours = int(system_crud.get_value(
                db_session,
                key="ARCHIVAL_TIMEOUT_HOURS",
                default=24
            ))
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—Å–µ—Ö Initial —Ç–æ–∫–µ–Ω–æ–≤
            await self._fetch_initial_tokens_metrics(initial_tokens, db_session)
            
            activated = 0
            archived = 0
            errors = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω
            for token in initial_tokens:
                try:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞—Ä—Ö–∏–≤–∞—Ü–∏—é –ø–æ —Ç–∞–π–º–∞—É—Ç—É
                    if self._should_archive_by_timeout(token, archival_timeout_hours):
                        await self._archive_token(
                            token, 
                            StatusChangeReason.ARCHIVAL_TIMEOUT,
                            f"Archived after {archival_timeout_hours}h in Initial status",
                            db_session
                        )
                        archived += 1
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–∫—Ç–∏–≤–∞—Ü–∏—é
                    if await self._should_activate_token(token, min_liquidity, min_tx_count, db_session):
                        await self._activate_token(
                            token,
                            StatusChangeReason.ACTIVATION,
                            f"Activated: liquidity‚â•${min_liquidity}, tx‚â•{min_tx_count}",
                            db_session
                        )
                        activated += 1
                        
                except Exception as e:
                    logger.error(f"Error processing Initial token {token.token_address}: {e}")
                    errors += 1
            
            self.stats["tokens_activated"] += activated
            self.stats["tokens_archived"] += archived
            self.stats["errors"] += errors
            self.stats["last_check"] = datetime.now()
            
            logger.info(f"Initial tokens monitoring completed: {activated} activated, {archived} archived, {errors} errors")
            
            return {
                "status": "success",
                "tokens_processed": len(initial_tokens),
                "activated": activated,
                "archived": archived,
                "errors": errors,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to monitor Initial tokens: {e}")
            self.stats["errors"] += 1
            return {
                "status": "error",
                "error": str(e)
            }

    async def monitor_active_tokens_lifecycle(self, db_session) -> Dict[str, Any]:
        """
        –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
        1. –ù–∏–∑–∫–∏–π —Å–∫–æ—Ä –¥–ª–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è -> –≤–æ–∑–≤—Ä–∞—Ç –≤ Initial
        2. –ù–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ –ø—É–ª–∞—Ö -> –≤–æ–∑–≤—Ä–∞—Ç –≤ Initial
        """
        try:
            from app.crud import token_crud, system_crud
            
            logger.info("Monitoring Active tokens lifecycle...")
            
            # –ü–æ–ª—É—á–∞–µ–º Active —Ç–æ–∫–µ–Ω—ã
            active_tokens = token_crud.get_by_status(
                db_session,
                status=TokenStatus.ACTIVE,
                limit=1000
            )
            
            self.stats["active_tokens_checked"] = len(active_tokens)
            
            if not active_tokens:
                return {
                    "status": "success",
                    "tokens_processed": 0,
                    "message": "No Active tokens found"
                }
            
            logger.info(f"Found {len(active_tokens)} Active tokens to check")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            min_score_threshold = float(system_crud.get_value(
                db_session,
                key="MIN_SCORE_THRESHOLD",
                default=0.5
            ))
            
            low_score_hours = int(system_crud.get_value(
                db_session,
                key="LOW_SCORE_HOURS",
                default=6
            ))
            
            min_tx_count = int(system_crud.get_value(
                db_session,
                key="MIN_TX_COUNT",
                default=300
            ))
            
            low_activity_checks = int(system_crud.get_value(
                db_session,
                key="LOW_ACTIVITY_CHECKS",
                default=10
            ))
            
            deactivated = 0
            errors = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –∞–∫—Ç–∏–≤–Ω—ã–π —Ç–æ–∫–µ–Ω
            for token in active_tokens:
                try:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∏–∑–∫–∏–π —Å–∫–æ—Ä
                    if await self._should_deactivate_by_score(
                        token, min_score_threshold, low_score_hours, db_session
                    ):
                        await self._deactivate_token(
                            token,
                            StatusChangeReason.LOW_SCORE,
                            f"Deactivated: score<{min_score_threshold} for {low_score_hours}h",
                            db_session
                        )
                        deactivated += 1
                        continue
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∏–∑–∫—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
                    if await self._should_deactivate_by_activity(
                        token, min_tx_count, low_activity_checks, db_session
                    ):
                        await self._deactivate_token(
                            token,
                            StatusChangeReason.LOW_ACTIVITY,
                            f"Deactivated: tx<{min_tx_count} for {low_activity_checks} checks",
                            db_session
                        )
                        deactivated += 1
                        
                except Exception as e:
                    logger.error(f"Error processing Active token {token.token_address}: {e}")
                    errors += 1
            
            self.stats["tokens_deactivated"] += deactivated
            self.stats["errors"] += errors
            self.stats["last_check"] = datetime.now()
            
            logger.info(f"Active tokens lifecycle completed: {deactivated} deactivated, {errors} errors")
            
            return {
                "status": "success",
                "tokens_processed": len(active_tokens),
                "deactivated": deactivated,
                "errors": errors,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to monitor Active tokens lifecycle: {e}")
            self.stats["errors"] += 1
            return {
                "status": "error",
                "error": str(e)
            }

    async def _fetch_initial_tokens_metrics(self, tokens: List, db_session):
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è Initial —Ç–æ–∫–µ–Ω–æ–≤
        """
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Birdeye –º–µ–Ω–µ–¥–∂–µ—Ä –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if not birdeye_manager.client:
                await birdeye_manager.initialize()
            
            successful = 0
            failed = 0
            
            for token in tokens:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π –¥–ª—è rate limits
                    success = await birdeye_manager.save_token_metrics(token.token_address)
                    
                    if success:
                        successful += 1
                    else:
                        failed += 1
                        
                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (–±–æ–ª—å—à–µ —á–µ–º –¥–ª—è Active)
                    import asyncio
                    await asyncio.sleep(1.0)  # 1 —Å–µ–∫—É–Ω–¥–∞ –¥–ª—è Initial —Ç–æ–∫–µ–Ω–æ–≤
                    
                except Exception as e:
                    logger.error(f"Error fetching metrics for Initial token {token.token_address}: {e}")
                    failed += 1
            
            logger.info(f"Initial tokens metrics: {successful} successful, {failed} failed")
            
        except Exception as e:
            logger.error(f"Failed to fetch Initial tokens metrics: {e}")

    def _should_archive_by_timeout(self, token, timeout_hours: int) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏–≤–∞—Ü–∏–∏ –ø–æ —Ç–∞–π–º–∞—É—Ç—É
        """
        if token.status != TokenStatus.INITIAL:
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –≤ —Å—Ç–∞—Ç—É—Å–µ Initial
        time_in_initial = datetime.now() - token.created_at.replace(tzinfo=None)
        
        return time_in_initial.total_seconds() > (timeout_hours * 3600)

    async def _should_activate_token(
        self, 
        token, 
        min_liquidity: float, 
        min_tx_count: int,
        db_session
    ) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–∞
        
        –£—Å–ª–æ–≤–∏—è:
        1. –ï—Å—Ç—å –ø—É–ª —Å –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å—é ‚â• min_liquidity
        2. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π ‚â• min_tx_count
        """
        try:
            from app.crud import token_metrics_crud, pool_crud
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—É–ª–æ–≤
            pools = pool_crud.get_by_token(db_session, token_id=token.id)
            if not pools:
                return False
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            latest_metrics = token_metrics_crud.get_latest(db_session, token_id=token.id)
            if not latest_metrics:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç—å
            if float(latest_metrics.liquidity_usd) < min_liquidity:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
            if latest_metrics.tx_count_1h < min_tx_count:
                return False
            
            logger.info(
                f"Token {token.token_address[:8]}... meets activation criteria: "
                f"liquidity=${latest_metrics.liquidity_usd}, tx={latest_metrics.tx_count_1h}"
            )
            
            return True
            
        except Exception as e:
            logger.error(f"Error checking activation conditions for {token.token_address}: {e}")
            return False

    async def _should_deactivate_by_score(
        self, 
        token, 
        min_score: float, 
        low_score_hours: int,
        db_session
    ) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–æ –Ω–∏–∑–∫–æ–º—É —Å–∫–æ—Ä—É
        """
        try:
            from app.crud import token_scores_crud
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–∫–æ—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥
            cutoff_time = datetime.now() - timedelta(hours=low_score_hours)
            
            scores = token_scores_crud.get_by_token(
                db_session,
                token_id=token.id,
                hours_back=low_score_hours,
                limit=1000
            )
            
            if not scores:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –≤—Å–µ —Å–∫–æ—Ä—ã –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞
            low_scores = [s for s in scores if s.score_value < min_score]
            
            # –ï—Å–ª–∏ –≤—Å–µ —Å–∫–æ—Ä—ã –∑–∞ –ø–µ—Ä–∏–æ–¥ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞
            if len(low_scores) == len(scores) and len(scores) >= 5:  # –ú–∏–Ω–∏–º—É–º 5 –∏–∑–º–µ—Ä–µ–Ω–∏–π
                logger.info(
                    f"Token {token.token_address[:8]}... has low score for {low_score_hours}h: "
                    f"avg={sum(s.score_value for s in scores)/len(scores):.3f} < {min_score}"
                )
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking score deactivation for {token.token_address}: {e}")
            return False

    async def _should_deactivate_by_activity(
        self, 
        token, 
        min_tx_count: int, 
        low_activity_checks: int,
        db_session
    ) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–µ–∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–æ –Ω–∏–∑–∫–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        """
        try:
            from app.crud import token_metrics_crud
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            recent_metrics = token_metrics_crud.get_by_token(
                db_session,
                token_id=token.id,
                hours_back=low_activity_checks,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤
                limit=low_activity_checks
            )
            
            if len(recent_metrics) < low_activity_checks:
                return False  # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∏–∑–∫–∞—è
            low_activity_count = 0
            for metrics in recent_metrics:
                if metrics.tx_count_1h < min_tx_count:
                    low_activity_count += 1
            
            # –ï—Å–ª–∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø—Ä–æ–≤–µ—Ä–æ–∫ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∏–∑–∫—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            if low_activity_count >= (low_activity_checks * 0.8):  # 80% –ø—Ä–æ–≤–µ—Ä–æ–∫
                logger.info(
                    f"Token {token.token_address[:8]}... has low activity: "
                    f"{low_activity_count}/{low_activity_checks} checks below {min_tx_count} tx"
                )
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking activity deactivation for {token.token_address}: {e}")
            return False

    async def _activate_token(
        self, 
        token, 
        reason: StatusChangeReason,
        metadata: str,
        db_session
    ):
        """
        –ê–∫—Ç–∏–≤–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∞
        """
        try:
            from app.crud import token_crud
            
            updated_token = token_crud.update_status(
                db_session,
                token=token,
                new_status=TokenStatus.ACTIVE,
                reason=reason,
                metadata=metadata
            )
            
            logger.info(
                f"‚úÖ Token activated: {token.token_address[:8]}...",
                extra={
                    "token_id": str(token.id),
                    "token_address": token.token_address,
                    "old_status": "initial",
                    "new_status": "active",
                    "reason": reason.value,
                    "metadata": metadata
                }
            )
            
        except Exception as e:
            logger.error(f"Failed to activate token {token.token_address}: {e}")
            raise

    async def _archive_token(
        self, 
        token, 
        reason: StatusChangeReason,
        metadata: str,
        db_session
    ):
        """
        –ê—Ä—Ö–∏–≤–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∞
        """
        try:
            from app.crud import token_crud
            
            updated_token = token_crud.update_status(
                db_session,
                token=token,
                new_status=TokenStatus.ARCHIVED,
                reason=reason,
                metadata=metadata
            )
            
            logger.info(
                f"üì¶ Token archived: {token.token_address[:8]}...",
                extra={
                    "token_id": str(token.id),
                    "token_address": token.token_address,
                    "old_status": token.status.value,
                    "new_status": "archived",
                    "reason": reason.value,
                    "metadata": metadata
                }
            )
            
        except Exception as e:
            logger.error(f"Failed to archive token {token.token_address}: {e}")
            raise

    async def _deactivate_token(
        self, 
        token, 
        reason: StatusChangeReason,
        metadata: str,
        db_session
    ):
        """
        –î–µ–∞–∫—Ç–∏–≤–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∞ (–≤–æ–∑–≤—Ä–∞—Ç –≤ Initial)
        """
        try:
            from app.crud import token_crud
            
            updated_token = token_crud.update_status(
                db_session,
                token=token,
                new_status=TokenStatus.INITIAL,
                reason=reason,
                metadata=metadata
            )
            
            logger.info(
                f"‚¨áÔ∏è Token deactivated: {token.token_address[:8]}...",
                extra={
                    "token_id": str(token.id),
                    "token_address": token.token_address,
                    "old_status": "active",
                    "new_status": "initial",
                    "reason": reason.value,
                    "metadata": metadata
                }
            )
            
        except Exception as e:
            logger.error(f"Failed to deactivate token {token.token_address}: {e}")
            raise

    async def get_lifecycle_stats(self, db_session) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
        """
        try:
            from app.crud import token_crud
            from app.models.token import TokenStatusHistory
            from sqlalchemy import func, desc
            
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
            stats = token_crud.get_stats(db_session)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
            cutoff_time = datetime.now() - timedelta(hours=24)
            
            transitions_24h = db_session.query(
                TokenStatusHistory.reason,
                func.count(TokenStatusHistory.id).label('count')
            ).filter(
                TokenStatusHistory.changed_at >= cutoff_time
            ).group_by(TokenStatusHistory.reason).all()
            
            transitions_data = {}
            for transition in transitions_24h:
                transitions_data[transition.reason.value] = transition.count
            
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø–µ—Ä–µ—Ö–æ–¥—ã
            recent_transitions = db_session.query(TokenStatusHistory).order_by(
                desc(TokenStatusHistory.changed_at)
            ).limit(10).all()
            
            recent_data = []
            for transition in recent_transitions:
                token = token_crud.get(db_session, id=transition.token_id)
                recent_data.append({
                    "token_address": token.token_address if token else "unknown",
                    "old_status": transition.old_status.value if transition.old_status else None,
                    "new_status": transition.new_status.value,
                    "reason": transition.reason.value,
                    "changed_at": transition.changed_at.isoformat(),
                    "metadata": transition.change_metadata
                })
            
            result = {
                "token_stats": stats,
                "lifecycle_stats": self.stats,
                "transitions_24h": transitions_data,
                "recent_transitions": recent_data,
                "timestamp": datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Failed to get lifecycle stats: {e}")
            return {
                "error": str(e)
            }

    def get_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
        """
        return self.stats.copy()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞
lifecycle_manager = TokenLifecycleManager()
